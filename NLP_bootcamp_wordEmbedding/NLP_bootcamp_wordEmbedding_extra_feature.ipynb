{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I explore adding variance of dense representation of words after removing stopwords. I also change the structre of the model as to seprate parts of inputs that will connect directly to the first Dense layer rather than go through the Embedding layer which didn't make a lot of sence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "7B8laNKfvzzp"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "72I93EcZwXzA"
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(r\"..\\NLP_Bootcamp\\Train_Dataset.csv\")\n",
    "test_data = pd.read_csv(r\"..\\NLP_Bootcamp\\Test_Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "jiUGDeUjwiJC",
    "outputId": "fcd5d41c-464b-44b4-8344-7e4798aa6d6d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>supreme court votes 7-2 to legalize all worldl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hungover man horrified to learn he made dozens...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>emily's list founder: women are the 'problem s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>send your kids back to school with confidence</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>watch: experts talk pesticides and health</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline  is_sarcastic\n",
       "0  supreme court votes 7-2 to legalize all worldl...             1\n",
       "1  hungover man horrified to learn he made dozens...             1\n",
       "2  emily's list founder: women are the 'problem s...             0\n",
       "3      send your kids back to school with confidence             0\n",
       "4          watch: experts talk pesticides and health             0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tuWWiapSw85P",
    "outputId": "790afbc4-8008-41fa-a67d-0b60d04a8151"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    23958\n",
       "1    20304\n",
       "Name: is_sarcastic, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"is_sarcastic\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cplrx_8ZxHkM"
   },
   "source": [
    "almost balanced training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RiFwN4aXwnKU"
   },
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B5NIxHffzKLb"
   },
   "source": [
    "removing contractions: emily's -> emily is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "eDmkoRkLzVYM"
   },
   "outputs": [],
   "source": [
    "import contractions\n",
    "train_data[\"headline\"] = train_data[\"headline\"].apply(contractions.fix)\n",
    "\n",
    "test_data[\"headline\"] = test_data[\"headline\"].apply(contractions.fix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pH1-Rrzbxt61"
   },
   "source": [
    "Removing Special Characters and Symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "aAdplQ3AwmwA"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]'\n",
    "    text = re.sub(pattern, ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "opVhFGjcxz-i"
   },
   "outputs": [],
   "source": [
    "train_data[\"headline\"] = train_data[\"headline\"].apply(remove_special_characters)\n",
    "\n",
    "test_data[\"headline\"] = test_data[\"headline\"].apply(remove_special_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>supreme court votes 7 2 to legalize all worldl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hungover man horrified to learn he made dozens...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>emily s list founder  women are the  problem s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>send your kids back to school with confidence</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>watch  experts talk pesticides and health</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline  is_sarcastic\n",
       "0  supreme court votes 7 2 to legalize all worldl...             1\n",
       "1  hungover man horrified to learn he made dozens...             1\n",
       "2  emily s list founder  women are the  problem s...             0\n",
       "3      send your kids back to school with confidence             0\n",
       "4          watch  experts talk pesticides and health             0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rty87Hxn4iP0"
   },
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "PKA4zlyE1h2h"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "W4agtQD35SI_"
   },
   "outputs": [],
   "source": [
    "X_train, X_val, Y_train, Y_val = train_test_split(train_data[[\"headline\"]], train_data[\"is_sarcastic\"], test_size=0.2, shuffle= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6hwAw3_k4oOl"
   },
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#### Simple model using ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "Oh3IGNBs7I8z"
   },
   "outputs": [],
   "source": [
    "import textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "vp-XUsWI5ry0"
   },
   "outputs": [],
   "source": [
    "#creating statistical features and only choosing relevant ones\n",
    "import string\n",
    "\n",
    "X_train['char_count'] = X_train['headline'].apply(len)\n",
    "X_train['word_count'] = X_train['headline'].apply(lambda x: len(x.split()))\n",
    "#X_train['word_density'] = X_train['char_count'] / (X_train['word_count']+1)\n",
    "#x_train_snt_obj = X_train['headline'].apply(lambda row: textblob.TextBlob(row).sentiment)\n",
    "#X_train['Polarity'] = [obj.polarity for obj in x_train_snt_obj.values]\n",
    "#X_train['Subjectivity'] = [obj.subjectivity for obj in x_train_snt_obj.values]\n",
    "\n",
    "max_char_count = np.max(X_train['char_count'])\n",
    "max_word_count = np.max(X_train['word_count'])\n",
    "\n",
    "X_train['char_count'] = X_train['char_count'] / max_char_count\n",
    "X_train['word_count'] = X_train['word_count'] / max_word_count\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_val['char_count'] = X_val['headline'].apply(len)\n",
    "X_val['word_count'] = X_val['headline'].apply(lambda x: len(x.split()))\n",
    "X_val['char_count'] = X_val['char_count'] / max_char_count\n",
    "X_val['word_count'] = X_val['word_count'] / max_word_count\n",
    "\n",
    "#X_val['word_density'] = X_val['char_count'] / (X_val['word_count']+1)\n",
    "#X_val_snt_obj = X_val['headline'].apply(lambda row: textblob.TextBlob(row).sentiment)\n",
    "#X_val['Polarity'] = [obj.polarity for obj in X_val_snt_obj.values]\n",
    "#X_val['Subjectivity'] = [obj.subjectivity for obj in X_val_snt_obj.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "1T49Rkrb52NJ",
    "outputId": "7f541b24-7e68-4b54-c3df-9dcc367004dd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>supreme court votes 7 2 to legalize all worldl...</td>\n",
       "      <td>0.057235</td>\n",
       "      <td>0.065789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hungover man horrified to learn he made dozens...</td>\n",
       "      <td>0.071274</td>\n",
       "      <td>0.078947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>emily s list founder  women are the  problem s...</td>\n",
       "      <td>0.070194</td>\n",
       "      <td>0.072368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>send your kids back to school with confidence</td>\n",
       "      <td>0.048596</td>\n",
       "      <td>0.052632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>watch  experts talk pesticides and health</td>\n",
       "      <td>0.044276</td>\n",
       "      <td>0.039474</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline  char_count  word_count\n",
       "0  supreme court votes 7 2 to legalize all worldl...    0.057235    0.065789\n",
       "1  hungover man horrified to learn he made dozens...    0.071274    0.078947\n",
       "2  emily s list founder  women are the  problem s...    0.070194    0.072368\n",
       "3      send your kids back to school with confidence    0.048596    0.052632\n",
       "4          watch  experts talk pesticides and health    0.044276    0.039474"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "8Wj_0opmYGju"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "#loading glove pretrained vectors\n",
    "\n",
    "path_to_glove_file = r\"..\\glove.6B\\glove.6B.300d.txt\"\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(path_to_glove_file, encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35409, 152) (8853, 152) <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "24565"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#constructing new embedding features\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(X_train[\"headline\"])\n",
    "\n",
    "encoded_train = t.texts_to_sequences(X_train[\"headline\"])\n",
    "encoded_val = t.texts_to_sequences(X_val[\"headline\"])\n",
    "\n",
    "max_length = len(max(encoded_train, key= lambda x: len(x)))\n",
    "\n",
    "padded_train = pad_sequences(encoded_train,\n",
    "                             maxlen = max_length,\n",
    "                             padding = \"post\")\n",
    "\n",
    "padded_val = pad_sequences(encoded_val,\n",
    "                           maxlen = max_length,\n",
    "                           padding = \"post\")\n",
    "\n",
    "print(padded_train.shape, padded_val.shape, type(padded_train))\n",
    "\n",
    "vocab_size = len(t.word_index) + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\33752\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\33752\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "def remove_stopwords(text, is_lower_case=False, stopwords=None):\n",
    "    if not stopwords:\n",
    "        stopwords = nltk.corpus.stopwords.words('english')\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    \n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopwords]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopwords]\n",
    "    \n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>processed_headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>supreme court votes 7 2 to legalize all worldl...</td>\n",
       "      <td>0.057235</td>\n",
       "      <td>0.065789</td>\n",
       "      <td>supreme court votes legalize worldly vices</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hungover man horrified to learn he made dozens...</td>\n",
       "      <td>0.071274</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>hungover man horrified learn made dozens plans...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>emily s list founder  women are the  problem s...</td>\n",
       "      <td>0.070194</td>\n",
       "      <td>0.072368</td>\n",
       "      <td>emily list founder women problem solvers congress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>send your kids back to school with confidence</td>\n",
       "      <td>0.048596</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>send kids back school confidence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>watch  experts talk pesticides and health</td>\n",
       "      <td>0.044276</td>\n",
       "      <td>0.039474</td>\n",
       "      <td>watch experts talk pesticides health</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline  char_count  word_count  \\\n",
       "0  supreme court votes 7 2 to legalize all worldl...    0.057235    0.065789   \n",
       "1  hungover man horrified to learn he made dozens...    0.071274    0.078947   \n",
       "2  emily s list founder  women are the  problem s...    0.070194    0.072368   \n",
       "3      send your kids back to school with confidence    0.048596    0.052632   \n",
       "4          watch  experts talk pesticides and health    0.044276    0.039474   \n",
       "\n",
       "                                  processed_headline  \n",
       "0         supreme court votes legalize worldly vices  \n",
       "1  hungover man horrified learn made dozens plans...  \n",
       "2  emily list founder women problem solvers congress  \n",
       "3                   send kids back school confidence  \n",
       "4               watch experts talk pesticides health  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[\"processed_headline\"] = X_train[\"headline\"].apply(remove_special_characters, remove_digits= True)\n",
    "X_train[\"processed_headline\"] = X_train[\"processed_headline\"].apply(remove_stopwords)\n",
    "\n",
    "\n",
    "X_val[\"processed_headline\"] = X_val[\"headline\"].apply(remove_special_characters, remove_digits= True)\n",
    "X_val[\"processed_headline\"] = X_val[\"processed_headline\"].apply(remove_stopwords)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>processed_headline</th>\n",
       "      <th>variance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>supreme court votes 7 2 to legalize all worldl...</td>\n",
       "      <td>0.057235</td>\n",
       "      <td>0.065789</td>\n",
       "      <td>supreme court votes legalize worldly vices</td>\n",
       "      <td>0.624866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hungover man horrified to learn he made dozens...</td>\n",
       "      <td>0.071274</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>hungover man horrified learn made dozens plans...</td>\n",
       "      <td>0.425083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>emily s list founder  women are the  problem s...</td>\n",
       "      <td>0.070194</td>\n",
       "      <td>0.072368</td>\n",
       "      <td>emily list founder women problem solvers congress</td>\n",
       "      <td>0.538932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>send your kids back to school with confidence</td>\n",
       "      <td>0.048596</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>send kids back school confidence</td>\n",
       "      <td>0.511886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>watch  experts talk pesticides and health</td>\n",
       "      <td>0.044276</td>\n",
       "      <td>0.039474</td>\n",
       "      <td>watch experts talk pesticides health</td>\n",
       "      <td>0.499833</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline  char_count  word_count  \\\n",
       "0  supreme court votes 7 2 to legalize all worldl...    0.057235    0.065789   \n",
       "1  hungover man horrified to learn he made dozens...    0.071274    0.078947   \n",
       "2  emily s list founder  women are the  problem s...    0.070194    0.072368   \n",
       "3      send your kids back to school with confidence    0.048596    0.052632   \n",
       "4          watch  experts talk pesticides and health    0.044276    0.039474   \n",
       "\n",
       "                                  processed_headline  variance  \n",
       "0         supreme court votes legalize worldly vices  0.624866  \n",
       "1  hungover man horrified learn made dozens plans...  0.425083  \n",
       "2  emily list founder women problem solvers congress  0.538932  \n",
       "3                   send kids back school confidence  0.511886  \n",
       "4               watch experts talk pesticides health  0.499833  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def headline_variance(text):\n",
    "    epsilon = 0.0000001\n",
    "    tokens = text.split(\" \")\n",
    "    vectors = np.array([np.array(embeddings_index[token], dtype= np.float32) for token in tokens if token in embeddings_index.keys()])\n",
    "    if len(vectors) <= 1:\n",
    "        return epsilon\n",
    "    return np.var(vectors)\n",
    "\n",
    "X_train[\"variance\"] = X_train[\"processed_headline\"].apply(headline_variance)\n",
    "\n",
    "max_variance = np.max(X_train[\"variance\"])\n",
    "X_train[\"variance\"] = X_train[\"variance\"] / max_variance\n",
    "\n",
    "X_val[\"variance\"] = X_val[\"processed_headline\"].apply(headline_variance)\n",
    "X_val[\"variance\"] = X_val[\"variance\"] / max_variance\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "headline              0\n",
       "char_count            0\n",
       "word_count            0\n",
       "processed_headline    0\n",
       "variance              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24565, 300)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_vector_length = len(next(iter(embeddings_index.values())))\n",
    "embedding_matrix = np.zeros((vocab_size, dense_vector_length)) # vector len of each word is 300\n",
    "\n",
    "for word, i in t.word_index.items():\n",
    "    if word in embeddings_index.keys():\n",
    "        vec = embeddings_index[word]\n",
    "        embedding_matrix[i] = vec\n",
    "\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35409, 155)\n",
      "(8853, 155)\n"
     ]
    }
   ],
   "source": [
    "#Merging the stastical features with embedding ones\n",
    "\n",
    "X_train_comb = np.concatenate((X_train.drop([\"headline\", \"processed_headline\"], axis=1), padded_train), axis=1)\n",
    "X_val_comb = np.concatenate((X_val.drop([\"headline\", \"processed_headline\"], axis=1), padded_val), axis=1)\n",
    "print(X_train_comb.shape)\n",
    "print(X_val_comb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up callbacks for the model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "\n",
    "def checkpoint_path():\n",
    "    return \"./model_extra_features/weights.{epoch:02d}-{val_accuracy:.4f}.hdf5\"\n",
    "\n",
    "def log_dir():\n",
    "    return \"./logs/fit/\" + datetime.now().strftime(\"%Y-%m-%d-%H:%M:%S\")\n",
    "\n",
    "earlystop = EarlyStopping(monitor = \"val_accuracy\", \n",
    "                          patience = 5, \n",
    "                          verbose = 1,  \n",
    "                          restore_best_weights = True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor = \"val_accuracy\", \n",
    "                              factor = .3,\n",
    "                              patience = 3,\n",
    "                              verbose = 1, \n",
    "                              min_delta = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input (InputLayer)             [(None, 155)]        0           []                               \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_2 (Sl  (None, 152)         0           ['input[0][0]']                  \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 152, 300)     7369500     ['tf.__operators__.getitem_2[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_3 (Sl  (None, 3)           0           ['input[0][0]']                  \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 45600)        0           ['embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 45603)        0           ['tf.__operators__.getitem_3[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 256)          11674624    ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 256)          0           ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 256)          0           ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 128)          32896       ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 128)          0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 32)           4128        ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 16)           528         ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 1)            17          ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 19,081,693\n",
      "Trainable params: 11,712,193\n",
      "Non-trainable params: 7,369,500\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#building the model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Input, Model\n",
    "\n",
    "\n",
    "input_layer = Input(shape = (X_train_comb.shape[1], ), name=\"input\")\n",
    "\n",
    "embedding = layers.Embedding(input_dim = vocab_size, \n",
    "                      output_dim = dense_vector_length, # glove vector size\n",
    "                      weights = [embedding_matrix], \n",
    "                      trainable = False)(input_layer[:,3:])\n",
    "\n",
    "\n",
    "flatten = layers.Flatten()(embedding)\n",
    "\n",
    "merge = layers.concatenate([input_layer[:,:3], flatten], axis= 1)\n",
    "\n",
    "dense = layers.Dense(256, activation = None, \n",
    "              kernel_initializer = \"he_uniform\")(merge)\n",
    "\n",
    "dropout = layers.Dropout(.35)(dense)\n",
    "activation = layers.Activation(\"relu\")(dropout)\n",
    "\n",
    "dense2 = layers.Dense(128, activation = 'relu')(activation)\n",
    "dropout2 = layers.Dropout(0.35)(dense2)\n",
    "\n",
    "dense3 = layers.Dense(32, activation = 'relu')(dropout2)\n",
    "\n",
    "dense4 = layers.Dense(16, activation= 'relu')(dense3)\n",
    "\n",
    "output = layers.Dense(1, activation = \"sigmoid\")(dense4)\n",
    "\n",
    "model = Model(inputs = input_layer, outputs = output)\n",
    "\n",
    "model.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "554/554 [==============================] - ETA: 0s - loss: 0.4474 - accuracy: 0.7864\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.83938, saving model to ./model_extra_features\\weights.01-0.8394.hdf5\n",
      "554/554 [==============================] - 25s 45ms/step - loss: 0.4474 - accuracy: 0.7864 - val_loss: 0.3663 - val_accuracy: 0.8394 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "553/554 [============================>.] - ETA: 0s - loss: 0.2649 - accuracy: 0.8938\n",
      "Epoch 00002: val_accuracy improved from 0.83938 to 0.90037, saving model to ./model_extra_features\\weights.02-0.9004.hdf5\n",
      "554/554 [==============================] - 25s 46ms/step - loss: 0.2651 - accuracy: 0.8938 - val_loss: 0.2564 - val_accuracy: 0.9004 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "553/554 [============================>.] - ETA: 0s - loss: 0.1438 - accuracy: 0.9455\n",
      "Epoch 00003: val_accuracy improved from 0.90037 to 0.92025, saving model to ./model_extra_features\\weights.03-0.9203.hdf5\n",
      "554/554 [==============================] - 25s 45ms/step - loss: 0.1437 - accuracy: 0.9455 - val_loss: 0.2294 - val_accuracy: 0.9203 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "554/554 [==============================] - ETA: 0s - loss: 0.0925 - accuracy: 0.9663\n",
      "Epoch 00004: val_accuracy improved from 0.92025 to 0.92500, saving model to ./model_extra_features\\weights.04-0.9250.hdf5\n",
      "554/554 [==============================] - 27s 48ms/step - loss: 0.0925 - accuracy: 0.9663 - val_loss: 0.2378 - val_accuracy: 0.9250 - lr: 0.0010\n",
      "Epoch 5/30\n",
      "553/554 [============================>.] - ETA: 0s - loss: 0.0674 - accuracy: 0.9763\n",
      "Epoch 00005: val_accuracy improved from 0.92500 to 0.92940, saving model to ./model_extra_features\\weights.05-0.9294.hdf5\n",
      "554/554 [==============================] - 28s 50ms/step - loss: 0.0674 - accuracy: 0.9763 - val_loss: 0.2399 - val_accuracy: 0.9294 - lr: 0.0010\n",
      "Epoch 6/30\n",
      "553/554 [============================>.] - ETA: 0s - loss: 0.0555 - accuracy: 0.9812\n",
      "Epoch 00006: val_accuracy improved from 0.92940 to 0.93155, saving model to ./model_extra_features\\weights.06-0.9315.hdf5\n",
      "554/554 [==============================] - 25s 46ms/step - loss: 0.0556 - accuracy: 0.9812 - val_loss: 0.2559 - val_accuracy: 0.9315 - lr: 0.0010\n",
      "Epoch 7/30\n",
      "553/554 [============================>.] - ETA: 0s - loss: 0.0473 - accuracy: 0.9836\n",
      "Epoch 00007: val_accuracy did not improve from 0.93155\n",
      "554/554 [==============================] - 26s 47ms/step - loss: 0.0473 - accuracy: 0.9836 - val_loss: 0.2581 - val_accuracy: 0.9313 - lr: 0.0010\n",
      "Epoch 8/30\n",
      "553/554 [============================>.] - ETA: 0s - loss: 0.0412 - accuracy: 0.9855\n",
      "Epoch 00008: val_accuracy did not improve from 0.93155\n",
      "554/554 [==============================] - 26s 48ms/step - loss: 0.0414 - accuracy: 0.9855 - val_loss: 0.2690 - val_accuracy: 0.9301 - lr: 0.0010\n",
      "Epoch 9/30\n",
      "553/554 [============================>.] - ETA: 0s - loss: 0.0414 - accuracy: 0.9853\n",
      "Epoch 00009: val_accuracy did not improve from 0.93155\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
      "554/554 [==============================] - 25s 45ms/step - loss: 0.0414 - accuracy: 0.9853 - val_loss: 0.2909 - val_accuracy: 0.9276 - lr: 0.0010\n",
      "Epoch 10/30\n",
      "554/554 [==============================] - ETA: 0s - loss: 0.0236 - accuracy: 0.9920\n",
      "Epoch 00010: val_accuracy improved from 0.93155 to 0.93211, saving model to ./model_extra_features\\weights.10-0.9321.hdf5\n",
      "554/554 [==============================] - 25s 45ms/step - loss: 0.0236 - accuracy: 0.9920 - val_loss: 0.3120 - val_accuracy: 0.9321 - lr: 3.0000e-04\n",
      "Epoch 11/30\n",
      "553/554 [============================>.] - ETA: 0s - loss: 0.0140 - accuracy: 0.9951\n",
      "Epoch 00011: val_accuracy did not improve from 0.93211\n",
      "554/554 [==============================] - 26s 47ms/step - loss: 0.0140 - accuracy: 0.9951 - val_loss: 0.3509 - val_accuracy: 0.9310 - lr: 3.0000e-04\n",
      "Epoch 12/30\n",
      "554/554 [==============================] - ETA: 0s - loss: 0.0115 - accuracy: 0.9955\n",
      "Epoch 00012: val_accuracy did not improve from 0.93211\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
      "554/554 [==============================] - 26s 46ms/step - loss: 0.0115 - accuracy: 0.9955 - val_loss: 0.3688 - val_accuracy: 0.9312 - lr: 3.0000e-04\n",
      "Epoch 13/30\n",
      "553/554 [============================>.] - ETA: 0s - loss: 0.0099 - accuracy: 0.9968\n",
      "Epoch 00013: val_accuracy did not improve from 0.93211\n",
      "554/554 [==============================] - 25s 45ms/step - loss: 0.0099 - accuracy: 0.9968 - val_loss: 0.3722 - val_accuracy: 0.9320 - lr: 9.0000e-05\n",
      "Epoch 14/30\n",
      "554/554 [==============================] - ETA: 0s - loss: 0.0091 - accuracy: 0.9966\n",
      "Epoch 00014: val_accuracy did not improve from 0.93211\n",
      "554/554 [==============================] - 25s 45ms/step - loss: 0.0091 - accuracy: 0.9966 - val_loss: 0.3761 - val_accuracy: 0.9318 - lr: 9.0000e-05\n",
      "Epoch 15/30\n",
      "553/554 [============================>.] - ETA: 0s - loss: 0.0082 - accuracy: 0.9972\n",
      "Epoch 00015: val_accuracy improved from 0.93211 to 0.93268, saving model to ./model_extra_features\\weights.15-0.9327.hdf5\n",
      "554/554 [==============================] - 25s 45ms/step - loss: 0.0082 - accuracy: 0.9972 - val_loss: 0.3751 - val_accuracy: 0.9327 - lr: 9.0000e-05\n",
      "Epoch 16/30\n",
      "553/554 [============================>.] - ETA: 0s - loss: 0.0071 - accuracy: 0.9975\n",
      "Epoch 00016: val_accuracy did not improve from 0.93268\n",
      "554/554 [==============================] - 25s 45ms/step - loss: 0.0071 - accuracy: 0.9975 - val_loss: 0.3794 - val_accuracy: 0.9325 - lr: 9.0000e-05\n",
      "Epoch 17/30\n",
      "553/554 [============================>.] - ETA: 0s - loss: 0.0071 - accuracy: 0.9977\n",
      "Epoch 00017: val_accuracy did not improve from 0.93268\n",
      "554/554 [==============================] - 25s 46ms/step - loss: 0.0071 - accuracy: 0.9977 - val_loss: 0.3855 - val_accuracy: 0.9325 - lr: 9.0000e-05\n",
      "Epoch 18/30\n",
      "553/554 [============================>.] - ETA: 0s - loss: 0.0060 - accuracy: 0.9981\n",
      "Epoch 00018: val_accuracy did not improve from 0.93268\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
      "554/554 [==============================] - 25s 46ms/step - loss: 0.0060 - accuracy: 0.9981 - val_loss: 0.3969 - val_accuracy: 0.9315 - lr: 9.0000e-05\n",
      "Epoch 19/30\n",
      "554/554 [==============================] - ETA: 0s - loss: 0.0065 - accuracy: 0.9981\n",
      "Epoch 00019: val_accuracy did not improve from 0.93268\n",
      "554/554 [==============================] - 27s 48ms/step - loss: 0.0065 - accuracy: 0.9981 - val_loss: 0.3970 - val_accuracy: 0.9326 - lr: 2.7000e-05\n",
      "Epoch 20/30\n",
      "553/554 [============================>.] - ETA: 0s - loss: 0.0057 - accuracy: 0.9981\n",
      "Epoch 00020: val_accuracy did not improve from 0.93268\n",
      "Restoring model weights from the end of the best epoch: 15.\n",
      "554/554 [==============================] - 26s 47ms/step - loss: 0.0057 - accuracy: 0.9981 - val_loss: 0.4023 - val_accuracy: 0.9326 - lr: 2.7000e-05\n",
      "Epoch 00020: early stopping\n"
     ]
    }
   ],
   "source": [
    "#training the model\n",
    "checkpoint = ModelCheckpoint(filepath = checkpoint_path(), \n",
    "                             monitor='val_accuracy', \n",
    "                             verbose = 1, \n",
    "                             save_best_only = True, \n",
    "                             mode = \"max\")\n",
    "\n",
    "callbacks_list = [checkpoint, earlystop, reduce_lr]\n",
    "\n",
    "history = model.fit(X_train_comb, Y_train, \n",
    "                    validation_data = (X_val_comb, Y_val), \n",
    "                    epochs = 30, \n",
    "                    batch_size = 64, \n",
    "                    callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.94      4765\n",
      "           1       0.93      0.92      0.93      4088\n",
      "\n",
      "    accuracy                           0.93      8853\n",
      "   macro avg       0.93      0.93      0.93      8853\n",
      "weighted avg       0.93      0.93      0.93      8853\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4502</td>\n",
       "      <td>263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>333</td>\n",
       "      <td>3755</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1\n",
       "0  4502   263\n",
       "1   333  3755"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#valuating the model\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "predictions = model.predict(X_val_comb)\n",
    "\n",
    "predictions = [1 if x > 0.5 else 0 for x in predictions]\n",
    "\n",
    "print(classification_report(Y_val, predictions))\n",
    "pd.DataFrame(confusion_matrix(Y_val, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we predict on our test_data so we start by applying the same transformaions we used on the training data\n",
    "X_test = test_data\n",
    "X_test['char_count'] = X_test['headline'].apply(len)\n",
    "X_test['word_count'] = X_test['headline'].apply(lambda x: len(x.split()))\n",
    "\n",
    "X_test['char_count'] = X_test['char_count'] / max_char_count\n",
    "X_test['word_count'] = X_test['word_count'] / max_word_count\n",
    "\n",
    "X_test[\"processed_headline\"] = X_test[\"headline\"].apply(remove_special_characters, remove_digits= True)\n",
    "X_test[\"processed_headline\"] = X_test[\"processed_headline\"].apply(remove_stopwords)\n",
    "\n",
    "X_test[\"variance\"] = X_test[\"processed_headline\"].apply(headline_variance)\n",
    "X_test[\"variance\"] = X_test[\"variance\"] / max_variance\n",
    "\n",
    "\n",
    "\n",
    "encoded_test = t.texts_to_sequences(X_test[\"headline\"])\n",
    "\n",
    "padded_test = pad_sequences(encoded_test,\n",
    "                           maxlen = max_length,\n",
    "                           padding = \"post\")\n",
    "\n",
    "X_test_comb = np.concatenate((X_test.drop([\"headline\", \"processed_headline\"], axis=1), padded_test), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "result = model.predict(X_test_comb)\n",
    "result = [1 if x > 0.5 else 0 for x in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file name=  prediction_results_12_12_2021_11.48.13.csv\n"
     ]
    }
   ],
   "source": [
    "#get time to not override different saves\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "\n",
    "dt_string = now.strftime(\"%d_%m_%Y_%H.%M.%S\")\n",
    "\n",
    "#save results localy\n",
    "res = pd.DataFrame(result)\n",
    "\n",
    "res.columns = ['prediction']\n",
    "res.to_csv(f\"prediction_results_{dt_string}.csv\", index = False) \n",
    "print(\"file name= \", f\"prediction_results_{dt_string}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It could be interesting to try and add the sum of distances between words in a headline as a feature after removing stop words like [to, if, and] etc. as generally in sarcastic headline we find words that usualy don't go together"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "NLP_bootcamp.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
