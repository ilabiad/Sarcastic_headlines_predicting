{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "7B8laNKfvzzp"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "72I93EcZwXzA"
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(r\"..\\NLP_Bootcamp\\Train_Dataset.csv\")\n",
    "test_data = pd.read_csv(r\"..\\NLP_Bootcamp\\Test_Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "jiUGDeUjwiJC",
    "outputId": "fcd5d41c-464b-44b4-8344-7e4798aa6d6d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>supreme court votes 7-2 to legalize all worldl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hungover man horrified to learn he made dozens...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>emily's list founder: women are the 'problem s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>send your kids back to school with confidence</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>watch: experts talk pesticides and health</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline  is_sarcastic\n",
       "0  supreme court votes 7-2 to legalize all worldl...             1\n",
       "1  hungover man horrified to learn he made dozens...             1\n",
       "2  emily's list founder: women are the 'problem s...             0\n",
       "3      send your kids back to school with confidence             0\n",
       "4          watch: experts talk pesticides and health             0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tuWWiapSw85P",
    "outputId": "790afbc4-8008-41fa-a67d-0b60d04a8151"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    23958\n",
       "1    20304\n",
       "Name: is_sarcastic, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"is_sarcastic\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cplrx_8ZxHkM"
   },
   "source": [
    "almost balanced training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RiFwN4aXwnKU"
   },
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B5NIxHffzKLb"
   },
   "source": [
    "removing contractions: emily's -> emily is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "eDmkoRkLzVYM"
   },
   "outputs": [],
   "source": [
    "import contractions\n",
    "train_data[\"headline\"] = train_data[\"headline\"].apply(contractions.fix)\n",
    "\n",
    "test_data[\"headline\"] = test_data[\"headline\"].apply(contractions.fix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pH1-Rrzbxt61"
   },
   "source": [
    "Removing Special Characters and Symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "aAdplQ3AwmwA"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]'\n",
    "    text = re.sub(pattern, ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "opVhFGjcxz-i"
   },
   "outputs": [],
   "source": [
    "train_data[\"headline\"] = train_data[\"headline\"].apply(remove_special_characters)\n",
    "\n",
    "test_data[\"headline\"] = test_data[\"headline\"].apply(remove_special_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>supreme court votes 7 2 to legalize all worldl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hungover man horrified to learn he made dozens...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>emily s list founder  women are the  problem s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>send your kids back to school with confidence</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>watch  experts talk pesticides and health</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline  is_sarcastic\n",
       "0  supreme court votes 7 2 to legalize all worldl...             1\n",
       "1  hungover man horrified to learn he made dozens...             1\n",
       "2  emily s list founder  women are the  problem s...             0\n",
       "3      send your kids back to school with confidence             0\n",
       "4          watch  experts talk pesticides and health             0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rty87Hxn4iP0"
   },
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "PKA4zlyE1h2h"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "W4agtQD35SI_"
   },
   "outputs": [],
   "source": [
    "X_train, X_val, Y_train, Y_val = train_test_split(train_data[[\"headline\"]], train_data[\"is_sarcastic\"], test_size=0.1, shuffle= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6hwAw3_k4oOl"
   },
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#### Simple model using ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Oh3IGNBs7I8z"
   },
   "outputs": [],
   "source": [
    "import textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "vp-XUsWI5ry0"
   },
   "outputs": [],
   "source": [
    "#creating statistical features and only choosing relevant ones\n",
    "import string\n",
    "\n",
    "X_train['char_count'] = X_train['headline'].apply(len)\n",
    "X_train['word_count'] = X_train['headline'].apply(lambda x: len(x.split()))\n",
    "#X_train['word_density'] = X_train['char_count'] / (X_train['word_count']+1)\n",
    "#x_train_snt_obj = X_train['headline'].apply(lambda row: textblob.TextBlob(row).sentiment)\n",
    "#X_train['Polarity'] = [obj.polarity for obj in x_train_snt_obj.values]\n",
    "#X_train['Subjectivity'] = [obj.subjectivity for obj in x_train_snt_obj.values]\n",
    "\n",
    "max_char_count = np.max(X_train['char_count'])\n",
    "max_word_count = np.max(X_train['word_count'])\n",
    "\n",
    "X_train['char_count'] = X_train['char_count'] / max_char_count\n",
    "X_train['word_count'] = X_train['word_count'] / max_word_count\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_val['char_count'] = X_val['headline'].apply(len)\n",
    "X_val['word_count'] = X_val['headline'].apply(lambda x: len(x.split()))\n",
    "X_val['char_count'] = X_val['char_count'] / max_char_count\n",
    "X_val['word_count'] = X_val['word_count'] / max_word_count\n",
    "\n",
    "#X_val['word_density'] = X_val['char_count'] / (X_val['word_count']+1)\n",
    "#X_val_snt_obj = X_val['headline'].apply(lambda row: textblob.TextBlob(row).sentiment)\n",
    "#X_val['Polarity'] = [obj.polarity for obj in X_val_snt_obj.values]\n",
    "#X_val['Subjectivity'] = [obj.subjectivity for obj in X_val_snt_obj.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "1T49Rkrb52NJ",
    "outputId": "7f541b24-7e68-4b54-c3df-9dcc367004dd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>supreme court votes 7 2 to legalize all worldl...</td>\n",
       "      <td>0.057235</td>\n",
       "      <td>0.065789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hungover man horrified to learn he made dozens...</td>\n",
       "      <td>0.071274</td>\n",
       "      <td>0.078947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>emily s list founder  women are the  problem s...</td>\n",
       "      <td>0.070194</td>\n",
       "      <td>0.072368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>send your kids back to school with confidence</td>\n",
       "      <td>0.048596</td>\n",
       "      <td>0.052632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>watch  experts talk pesticides and health</td>\n",
       "      <td>0.044276</td>\n",
       "      <td>0.039474</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline  char_count  word_count\n",
       "0  supreme court votes 7 2 to legalize all worldl...    0.057235    0.065789\n",
       "1  hungover man horrified to learn he made dozens...    0.071274    0.078947\n",
       "2  emily s list founder  women are the  problem s...    0.070194    0.072368\n",
       "3      send your kids back to school with confidence    0.048596    0.052632\n",
       "4          watch  experts talk pesticides and health    0.044276    0.039474"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "8Wj_0opmYGju"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "#loading glove pretrained vectors\n",
    "\n",
    "path_to_glove_file = r\"..\\glove.6B\\glove.6B.300d.txt\"\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(path_to_glove_file, encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39835, 152) (4427, 152) <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "25210"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#constructing new embedding features\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(X_train[\"headline\"])\n",
    "\n",
    "encoded_train = t.texts_to_sequences(X_train[\"headline\"])\n",
    "encoded_val = t.texts_to_sequences(X_val[\"headline\"])\n",
    "\n",
    "max_length = len(max(encoded_train, key= lambda x: len(x)))\n",
    "\n",
    "padded_train = pad_sequences(encoded_train,\n",
    "                             maxlen = max_length,\n",
    "                             padding = \"post\")\n",
    "\n",
    "padded_val = pad_sequences(encoded_val,\n",
    "                           maxlen = max_length,\n",
    "                           padding = \"post\")\n",
    "\n",
    "print(padded_train.shape, padded_val.shape, type(padded_train))\n",
    "\n",
    "vocab_size = len(t.word_index) + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25210, 300)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_vector_length = len(next(iter(embeddings_index.values())))\n",
    "embedding_matrix = np.zeros((vocab_size, dense_vector_length)) # vector len of each word is 300\n",
    "\n",
    "for word, i in t.word_index.items():\n",
    "    if word in embeddings_index.keys():\n",
    "        vec = embeddings_index[word]\n",
    "        embedding_matrix[i] = vec\n",
    "\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39835, 154)\n",
      "(4427, 154)\n"
     ]
    }
   ],
   "source": [
    "#Merging the stastical features with embedding ones\n",
    "\n",
    "X_train_comb = np.concatenate((X_train.drop(\"headline\", axis=1), padded_train), axis=1)\n",
    "X_val_comb = np.concatenate((X_val.drop(\"headline\", axis=1), padded_val), axis=1)\n",
    "print(X_train_comb.shape)\n",
    "print(X_val_comb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up callbacks for the model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "\n",
    "def checkpoint_path():\n",
    "    return \"./model/weights.{epoch:02d}-{val_accuracy:.4f}.hdf5\"\n",
    "\n",
    "def log_dir():\n",
    "    return \"./logs/fit/\" + datetime.now().strftime(\"%Y-%m-%d-%H:%M:%S\")\n",
    "\n",
    "earlystop = EarlyStopping(monitor = \"val_accuracy\", \n",
    "                          patience = 5, \n",
    "                          verbose = 1,  \n",
    "                          restore_best_weights = True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor = \"val_accuracy\", \n",
    "                              factor = .3,\n",
    "                              patience = 3,\n",
    "                              verbose = 1, \n",
    "                              min_delta = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 154)]             0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 154, 300)          7563000   \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 46200)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               11827456  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " activation (Activation)     (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                16448     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 16)                1040      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 19,407,961\n",
      "Trainable params: 11,844,961\n",
      "Non-trainable params: 7,563,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#building the model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Input, Model\n",
    "\n",
    "\n",
    "input_layer = Input(shape = (X_train_comb.shape[1], ), name=\"input\")\n",
    "\n",
    "embedding = layers.Embedding(input_dim = vocab_size, \n",
    "                      output_dim = dense_vector_length, # glove vector size\n",
    "                      weights = [embedding_matrix], \n",
    "                      trainable = False)(input_layer)\n",
    "\n",
    "\n",
    "flatten = layers.Flatten()(embedding)\n",
    "\n",
    "dense = layers.Dense(256, activation = None, \n",
    "              kernel_initializer = \"he_uniform\")(flatten)\n",
    "\n",
    "dropout = layers.Dropout(.25)(dense)\n",
    "activation = layers.Activation(\"relu\")(dropout)\n",
    "\n",
    "dense2 = layers.Dense(64, activation = 'relu')(activation)\n",
    "dropout2 = layers.Dropout(0.3)(dense2)\n",
    "\n",
    "dense3 = layers.Dense(16, activation = 'relu')(dropout2)\n",
    "\n",
    "output = layers.Dense(1, activation = \"sigmoid\")(dense3)\n",
    "\n",
    "model = Model(inputs = input_layer, outputs = output)\n",
    "\n",
    "model.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "311/312 [============================>.] - ETA: 0s - loss: 0.4363 - accuracy: 0.7946\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.85837, saving model to ./model\\weights.01-0.8584.hdf5\n",
      "312/312 [==============================] - 18s 56ms/step - loss: 0.4362 - accuracy: 0.7947 - val_loss: 0.3335 - val_accuracy: 0.8584 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "311/312 [============================>.] - ETA: 0s - loss: 0.2346 - accuracy: 0.9076\n",
      "Epoch 00002: val_accuracy improved from 0.85837 to 0.90648, saving model to ./model\\weights.02-0.9065.hdf5\n",
      "312/312 [==============================] - 16s 50ms/step - loss: 0.2345 - accuracy: 0.9076 - val_loss: 0.2418 - val_accuracy: 0.9065 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "312/312 [==============================] - ETA: 0s - loss: 0.1122 - accuracy: 0.9597\n",
      "Epoch 00003: val_accuracy improved from 0.90648 to 0.93020, saving model to ./model\\weights.03-0.9302.hdf5\n",
      "312/312 [==============================] - 15s 50ms/step - loss: 0.1122 - accuracy: 0.9597 - val_loss: 0.2230 - val_accuracy: 0.9302 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "311/312 [============================>.] - ETA: 0s - loss: 0.0659 - accuracy: 0.9767\n",
      "Epoch 00004: val_accuracy improved from 0.93020 to 0.93653, saving model to ./model\\weights.04-0.9365.hdf5\n",
      "312/312 [==============================] - 16s 50ms/step - loss: 0.0659 - accuracy: 0.9767 - val_loss: 0.2473 - val_accuracy: 0.9365 - lr: 0.0010\n",
      "Epoch 5/30\n",
      "312/312 [==============================] - ETA: 0s - loss: 0.0471 - accuracy: 0.9837\n",
      "Epoch 00005: val_accuracy improved from 0.93653 to 0.94150, saving model to ./model\\weights.05-0.9415.hdf5\n",
      "312/312 [==============================] - 16s 51ms/step - loss: 0.0471 - accuracy: 0.9837 - val_loss: 0.2531 - val_accuracy: 0.9415 - lr: 0.0010\n",
      "Epoch 6/30\n",
      "312/312 [==============================] - ETA: 0s - loss: 0.0405 - accuracy: 0.9855\n",
      "Epoch 00006: val_accuracy improved from 0.94150 to 0.94172, saving model to ./model\\weights.06-0.9417.hdf5\n",
      "312/312 [==============================] - 16s 50ms/step - loss: 0.0405 - accuracy: 0.9855 - val_loss: 0.2553 - val_accuracy: 0.9417 - lr: 0.0010\n",
      "Epoch 7/30\n",
      "311/312 [============================>.] - ETA: 0s - loss: 0.0345 - accuracy: 0.9877\n",
      "Epoch 00007: val_accuracy did not improve from 0.94172\n",
      "312/312 [==============================] - 16s 51ms/step - loss: 0.0345 - accuracy: 0.9877 - val_loss: 0.2626 - val_accuracy: 0.9406 - lr: 0.0010\n",
      "Epoch 8/30\n",
      "312/312 [==============================] - ETA: 0s - loss: 0.0294 - accuracy: 0.9901\n",
      "Epoch 00008: val_accuracy did not improve from 0.94172\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
      "312/312 [==============================] - 16s 51ms/step - loss: 0.0294 - accuracy: 0.9901 - val_loss: 0.2942 - val_accuracy: 0.9417 - lr: 0.0010\n",
      "Epoch 9/30\n",
      "311/312 [============================>.] - ETA: 0s - loss: 0.0155 - accuracy: 0.9946\n",
      "Epoch 00009: val_accuracy improved from 0.94172 to 0.94285, saving model to ./model\\weights.09-0.9429.hdf5\n",
      "312/312 [==============================] - 16s 50ms/step - loss: 0.0155 - accuracy: 0.9946 - val_loss: 0.3063 - val_accuracy: 0.9429 - lr: 3.0000e-04\n",
      "Epoch 10/30\n",
      "311/312 [============================>.] - ETA: 0s - loss: 0.0096 - accuracy: 0.9969\n",
      "Epoch 00010: val_accuracy improved from 0.94285 to 0.94556, saving model to ./model\\weights.10-0.9456.hdf5\n",
      "312/312 [==============================] - 16s 53ms/step - loss: 0.0096 - accuracy: 0.9969 - val_loss: 0.3149 - val_accuracy: 0.9456 - lr: 3.0000e-04\n",
      "Epoch 11/30\n",
      "311/312 [============================>.] - ETA: 0s - loss: 0.0072 - accuracy: 0.9978\n",
      "Epoch 00011: val_accuracy improved from 0.94556 to 0.94601, saving model to ./model\\weights.11-0.9460.hdf5\n",
      "312/312 [==============================] - 17s 53ms/step - loss: 0.0072 - accuracy: 0.9978 - val_loss: 0.3290 - val_accuracy: 0.9460 - lr: 3.0000e-04\n",
      "Epoch 12/30\n",
      "311/312 [============================>.] - ETA: 0s - loss: 0.0048 - accuracy: 0.9987\n",
      "Epoch 00012: val_accuracy did not improve from 0.94601\n",
      "312/312 [==============================] - 16s 52ms/step - loss: 0.0048 - accuracy: 0.9987 - val_loss: 0.3559 - val_accuracy: 0.9453 - lr: 3.0000e-04\n",
      "Epoch 13/30\n",
      "311/312 [============================>.] - ETA: 0s - loss: 0.0054 - accuracy: 0.9983\n",
      "Epoch 00013: val_accuracy did not improve from 0.94601\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
      "312/312 [==============================] - 16s 52ms/step - loss: 0.0054 - accuracy: 0.9983 - val_loss: 0.3549 - val_accuracy: 0.9449 - lr: 3.0000e-04\n",
      "Epoch 14/30\n",
      "312/312 [==============================] - ETA: 0s - loss: 0.0045 - accuracy: 0.9985\n",
      "Epoch 00014: val_accuracy did not improve from 0.94601\n",
      "312/312 [==============================] - 16s 52ms/step - loss: 0.0045 - accuracy: 0.9985 - val_loss: 0.3603 - val_accuracy: 0.9453 - lr: 9.0000e-05\n",
      "Epoch 15/30\n",
      "311/312 [============================>.] - ETA: 0s - loss: 0.0040 - accuracy: 0.9990\n",
      "Epoch 00015: val_accuracy did not improve from 0.94601\n",
      "312/312 [==============================] - 16s 52ms/step - loss: 0.0040 - accuracy: 0.9990 - val_loss: 0.3596 - val_accuracy: 0.9453 - lr: 9.0000e-05\n",
      "Epoch 16/30\n",
      "311/312 [============================>.] - ETA: 0s - loss: 0.0031 - accuracy: 0.9990\n",
      "Epoch 00016: val_accuracy improved from 0.94601 to 0.94624, saving model to ./model\\weights.16-0.9462.hdf5\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
      "312/312 [==============================] - 16s 50ms/step - loss: 0.0031 - accuracy: 0.9990 - val_loss: 0.3639 - val_accuracy: 0.9462 - lr: 9.0000e-05\n",
      "Epoch 17/30\n",
      "311/312 [============================>.] - ETA: 0s - loss: 0.0029 - accuracy: 0.9990\n",
      "Epoch 00017: val_accuracy did not improve from 0.94624\n",
      "312/312 [==============================] - 16s 51ms/step - loss: 0.0029 - accuracy: 0.9990 - val_loss: 0.3637 - val_accuracy: 0.9456 - lr: 2.7000e-05\n",
      "Epoch 18/30\n",
      "312/312 [==============================] - ETA: 0s - loss: 0.0029 - accuracy: 0.9991\n",
      "Epoch 00018: val_accuracy did not improve from 0.94624\n",
      "312/312 [==============================] - 16s 52ms/step - loss: 0.0029 - accuracy: 0.9991 - val_loss: 0.3659 - val_accuracy: 0.9458 - lr: 2.7000e-05\n",
      "Epoch 19/30\n",
      "312/312 [==============================] - ETA: 0s - loss: 0.0027 - accuracy: 0.9991\n",
      "Epoch 00019: val_accuracy did not improve from 0.94624\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 8.100000013655517e-06.\n",
      "312/312 [==============================] - 15s 48ms/step - loss: 0.0027 - accuracy: 0.9991 - val_loss: 0.3681 - val_accuracy: 0.9458 - lr: 2.7000e-05\n",
      "Epoch 20/30\n",
      "311/312 [============================>.] - ETA: 0s - loss: 0.0023 - accuracy: 0.9993\n",
      "Epoch 00020: val_accuracy did not improve from 0.94624\n",
      "312/312 [==============================] - 15s 48ms/step - loss: 0.0023 - accuracy: 0.9993 - val_loss: 0.3692 - val_accuracy: 0.9460 - lr: 8.1000e-06\n",
      "Epoch 21/30\n",
      "312/312 [==============================] - ETA: 0s - loss: 0.0023 - accuracy: 0.9994\n",
      "Epoch 00021: val_accuracy did not improve from 0.94624\n",
      "Restoring model weights from the end of the best epoch: 16.\n",
      "312/312 [==============================] - 15s 50ms/step - loss: 0.0023 - accuracy: 0.9994 - val_loss: 0.3704 - val_accuracy: 0.9460 - lr: 8.1000e-06\n",
      "Epoch 00021: early stopping\n"
     ]
    }
   ],
   "source": [
    "#training the model\n",
    "checkpoint = ModelCheckpoint(filepath = checkpoint_path(), \n",
    "                             monitor='val_accuracy', \n",
    "                             verbose = 1, \n",
    "                             save_best_only = True, \n",
    "                             mode = \"max\")\n",
    "\n",
    "callbacks_list = [checkpoint, earlystop, reduce_lr]\n",
    "\n",
    "history = model.fit(X_train_comb, Y_train, \n",
    "                    validation_data = (X_val_comb, Y_val), \n",
    "                    epochs = 30, \n",
    "                    batch_size = 128, \n",
    "                    callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.95      0.95      2339\n",
      "           1       0.95      0.94      0.94      2088\n",
      "\n",
      "    accuracy                           0.95      4427\n",
      "   macro avg       0.95      0.95      0.95      4427\n",
      "weighted avg       0.95      0.95      0.95      4427\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2232</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>131</td>\n",
       "      <td>1957</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1\n",
       "0  2232   107\n",
       "1   131  1957"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#valuating the model\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "predictions = model.predict(X_val_comb)\n",
    "\n",
    "predictions = [1 if x > 0.5 else 0 for x in predictions]\n",
    "\n",
    "print(classification_report(Y_val, predictions))\n",
    "pd.DataFrame(confusion_matrix(Y_val, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we predict on our test_data so we start by applying the same transformaions we used on the training data\n",
    "X_test = test_data\n",
    "X_test['char_count'] = X_test['headline'].apply(len)\n",
    "X_test['word_count'] = X_test['headline'].apply(lambda x: len(x.split()))\n",
    "\n",
    "X_test['char_count'] = X_test['char_count'] / max_char_count\n",
    "X_test['word_count'] = X_test['word_count'] / max_word_count\n",
    "\n",
    "\n",
    "encoded_test = t.texts_to_sequences(X_test[\"headline\"])\n",
    "\n",
    "padded_test = pad_sequences(encoded_test,\n",
    "                           maxlen = max_length,\n",
    "                           padding = \"post\")\n",
    "\n",
    "X_test_comb = np.concatenate((X_test.drop(\"headline\", axis=1), padded_test), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "result = model.predict(X_test_comb)\n",
    "result = [1 if x > 0.5 else 0 for x in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file name=  prediction_results_12_12_2021_11.36.46.csv\n"
     ]
    }
   ],
   "source": [
    "#get time to not override different saves\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "\n",
    "dt_string = now.strftime(\"%d_%m_%Y_%H.%M.%S\")\n",
    "\n",
    "#save results localy\n",
    "res = pd.DataFrame(result)\n",
    "\n",
    "res.columns = ['prediction']\n",
    "res.to_csv(f\"prediction_results_{dt_string}.csv\", index = False) \n",
    "print(\"file name= \", f\"prediction_results_{dt_string}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It could be interesting to try and add the sum of distances between words in a headline as a feature after removing stop words like [to, if, and] etc. as generally in sarcastic headline we find words that usualy don't go together"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "NLP_bootcamp.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
